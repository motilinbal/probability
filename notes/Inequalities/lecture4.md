# Page 0

שיעור 4 - 21-04-2025

בסעיף זה נלמד מספר אי שוויונות שימושיים.

1.4.1. אי-שוויוני ריכוז - Concentration inequalities

אי-שוויוני ריכוז אלו מספקים חסמים על ההסתברות שמשתנה מקרי יסטה מערכו הצפוי (או ערך אחר כגון החציון) במידה מסוימת. במילים אחרות, אי-שוויוני ריכוז מאפשרים לנו לכמת עד כמה משתנה מקרי "מרוכז" סביב ערך מסוים. אי-שוויוני ריכוז הם כלים חשובים מאוד בתורת ההסתברות, סטטיסטיקה, מדעי הנתונים (בפרט באלגוריתמים הסתברותיים ולמידת מכונה) ותחומים רבים אחרים. הם מאפשרים להוכיח תוצאות על התנהגות של משתנים מקריים ושל פונקציות שלהם, ולנתח את ההסתברות לאירועים חריגים.

דוגמאות מוכרות לאי-שוויוני ריכוז כוללות:

- אי-שוויון מרקוב :(Markov’s inequality) מספק חסם על ההסתברות של משתנה מקרי אי-שלילי להיות גדול מערך מסוים, בהינתן התוחלת שלו.
- אי-שוויון צ’בישב :(Chebyshev’s inequality) מספק חסם על ההסתברות של משתנה מקרי להיות רחוק מהתוחלת שלו במידה מסוימת, בהינתן התוחלת והשונות שלו.
- חסמי צ’רנוף :(Chernoff bounds) מספקים חסמים אקספוננציאליים על ההסתברות שסכום של משתנים מקריים בלתי תלויים (לרוב משתני ברנולי) יסטה באופן משמעותי מהתוחלת שלו. חסמים אלה חזקים יותר מאי-שוויונות מרקוב וצ’בישב במקרים רבים.
- אי-שוויון הופדינג :(Hoeffding’s inequality) דומה לחסמי צ’רנוף אך חל על סכום של משתנים מקריים חסומים.
- אי-שוויון אזומה-הופדינג :(Azuma-Hoeffding inequality) הכללה של אי-שוויון הופדינג עבור מרטינגלים עם הבדלים חסומים.

משפט 1.10 אי-שוויון מרקוב: (Markov’s inequality)

עבור משתנה מקרי X. 0≥ X

מוכחה:

יהיה משתנה מקרי X. 0≥ 0, אזי עבור x > 0

■X = E(I_{(X≥x)}X) + E(I_{(X<x)}X) ≥ E(I_{(X≥x)}X) ≥ E(I_{(X≥x)}).

האי השוויון הראשון נכון מכיוון ו- X. 0≥ 0, האי שוויון השני נכון מכיוון ו- X. 0≥ x, והשוויון האחרון נובע מכך ש-x הוא ערך קבוע.

עתה נעביר אגפים ונקבל את הנדרש.

# Page 1

מסקנה:

אם ל-X יש בנוסף מומנטים סופים, . אז זנב ההסתברות יורד בקצב שהוא לפחות פולינומי. זאת משום שלפי אי שוויון מרקוב:

ולכן לכל גדול כרצוננו . עדיין, אין צ'ביצ'ב שמאלה.

תוצאה של אי-שוויון צ'בישב: (Chebyshev's inequality) יהיה X משתנה מקרי עם , אזי

הוכחה:

נסמן ^{2}(. מכיוון שמדובר בחזקה ריבועית, הוא משתנה מקרי אי שלילי. לכל , לפי אי שוויון מרקוב,

המסקנה האחרונה נובעת מכך ש- ולכן (כדרש. כגדרש.

חשוב לציין כי אי שוויון צ'ביצ'ב אינו דורש ידע על ההתפלגות, רק על קיום המומנט/ים.

מסקנה:

יהיו משתנים מקריים שווי התפלגות לא מתואמים, ויהי . אפילו אם ההתפלגות של ידועה, בדרך כלל קשה למצוא את ההתפלגות של הממשע . אולם אם , אזי לפי אי שיוויון צ'ביצ'ב נקבל כי

בפרט, המשמעות היא שההתפלגות של מרכזת סביב . לפחות בקצב .

# Page 2

1.4.2 תוחלת וקמירות (Expectation and convexity)

הגדרה 1.12:

פונקציה R → A;נ היא קמורה על האינטרוול R ⊆ U אם לכל i = 1, x, y,

אם אי השוויון הפוך אז הפונקציה נקראת קעורה. ברור כי אם (־)g קמורה אז (־g)– קעורה, ולהיפך. ולכן כל תכונה של פונקציה קמורה מורחבת לפונקציה קעורה עם ההתאמות הנדרשות.

קמירות היא תכונה שימושית מאד.

למה 1.13:

פונקציה (־g) היא קמורה אם ורק אם קיימת פונקציה (x)v, כך ש

הוכחה:

נניח כי האי שוויון מתקיים, אז לכל y = 1,

נכפיל את המשוואה הראשונה ב-λ ואת השנייה ב- (1 – λ) ונחבר את שתי המשוואות, נקבל כי

והקמירות מתקבלת אם ניקח x = 1, x < 2) . נציב את x ונקבל ב]פרט שאגף ימין מתאפס:

עתה נעביר אגף ונקבל את אי השוויון.

מצד שני, אם (־g) g קמורה, אז לכל (0,1) = λ , מהגדרת הקמירות נקבל

עתה אם (־g) g גדירה, ניתן ל-λ לשאוף ל-0 ונקבל

כלומר קיבלנו את את השוויון הנדרש עם (v(x) = g'^{′}(x.

* כזכור, נגזרת של פונקציה בנקודה x מוגדרת ^^{2}כy =

גיאומטרית אי השוויון אומר כי הגרף של פונקציה קמורה שוכב מעל המשיק לגרף בכל נקודה.

פונקציות קמורות לא בהכרח גזירות, אבל אפשר להראות יש להן נגזרת מימין ונגזרת משמאל. לכן הטענה נותרת תקפה אם ניקח למשל, נגזרת מימין כאשר ניתן ל-λ לרדת ל-0 מימין - λ > 0 .

# Page 3

דוגמא 1.14: נניח (g) גזירה פעמיים, אזי

$$
\begin{gathered}
g(y)-g(x)=\int_{x}^{y} g^{\prime}(u) d u=\int_{x}^{y} g^{\prime}(x) d u+\int_{x}^{y}\left(g^{\prime}(u)-g^{\prime}(x)\right) d u= \\
g^{\prime}(x)(y-x)+\int_{x}^{y}\left(g^{\prime}(u)-g^{\prime}(x)\right) d u= \\
g^{\prime}(x)(y-x)+\int_{x}^{y} \int_{x}^{u} g^{\prime \prime}(v) d v d u
\end{gathered}
$$

לפי למה 1.13, g(י) קמורה אם ורק אם הביטוי הימני הוא חיובי לכל y-1 x, מה שאומר כי g'’(v) ≥ 0 לכל v ו- v. זוהי דרך נוחה להוכיח קמירות. למשל g(x) = e^{x} , g(x) = x^{p} p ≥ 2 ועוד.

משפט 1.15 **אי שוויון ג'נסן** (Jensen inequality).)
תהי (g (y פונקציה קמורה ויהי X משתנה מקרי עם תוחלת סופית, אזי

$$
\mathbb{E} g(X) \geq g(\mathbb{E} X)
$$

הוכחה:
מלמה 1.13 נקבל כי קיימת פונקציה (v(x כך ש

$$
g(X)-g(\mathbb{E} X) \geq v(\mathbb{E} X)(X-\mathbb{E} X)
$$

עתה ניקח תוחלת משני צידי האי שוויון ונקבל

$$
\mathbb{E}[g(X)-g(\mathbb{E} X)]=\mathbb{E} g(X)-g(\mathbb{E} X) \geq \mathbb{E}[v(\mathbb{E} X)(X-\mathbb{E} X)]=v(\mathbb{E} X) \mathbb{E}[(X-\mathbb{E} X)]=0
$$

$\mathbb{E} g(X)-g(\mathbb{E} X) \geq 0 \Rightarrow \mathbb{E} g(X) \geq g(\mathbb{E} X)$
כנדרש.

אי שוויון ג'נסן משמש להוכחת אי שוויוניות חשובים נוספים.

משפט 1.16 **אי שוויון ליאפומנב** (Lyapunov inequality)
יהיה X משתנה מקרי. עבור p ≥ q ,

$$
\left(\mathbb{E}|X|^{q}\right)^{1 / q} \leq\left(\mathbb{E}|X|^{p}\right)^{1 / p}
$$

הפונקציה g(x) = x^{r} r < 1 קעורה על R_{+} , מכיוון ש: g'’(x) = r(r - 1) x^{r-2} < 0 לכל x > 0 .

יהי q, אזי לפי אי שוויון ג'נסן על פונקציות קמורות:

געלה את שני צידי האי שוויון בחזקת 1/q ונקבל את אי השוויון הנדרש:

(געלה 1/q)

# Page 4

מסקנה 1.17

יהיה משתנה מקרי כך ש- 0 < |E|X|P < 0 < |E| עבור 0 < p כלשהו, אזי 0 < |E|X לכל q < q.

הוכחה: נובע ישירות מאי שוויון ליאפומוב.

בשל קוצר הזמן, אין אפשרות להציג את השימושים שנעשים בחומר הנלמד במהלך השיעור.

הקישור הבא מוביל לשאלה שהפניתי למודל perplexity AI תוכלו למצוא דוגמאות והפניות למקורות המדגימים את השימושים הרבים שנעשים באי שוויון ג'נסן בכלכלה, תורת האינפורמציה, אופטימיזציה ועוד, ולהמשיך ולחקור את הנושא בעזרת המודל והשאלות הקשורות שהוא מציע.

https://www.perplexity.ai/search/what-are-the-uses-of-of-jensen-uTJ3oIALRGCAxF8OIp_Yw#0 במסמך "What are the uses of Jensen Inequality" אפשר לקרוא סיכום ראשוני של תשובות המודל.

תרגיל (לא הוצג בשיעור)

הוכיחו כי עבור E[ln(X)] ≤ ln[E[X . (דוגמא ליעילות לוגריתמית)

תשובה:

הפונקציה הלוגריתמית היא פונקציה קעורה (נגזרת שנייה שלילית), ולכן אי השוויון נובע מאי שוויון ג'נסן עבור פונקציות קעורות.

#### סעיף 1.5: דגימה של משתנה מקרי

דגימת משתנה מקרי משמעותה להסתכל על מימוש מסוים של הניסוי המקרי. חבילות תוכנה כמו R, פייתון וכו' כוללות בתוכן פונקציות לדגימה מהתפלגויות שונות.

למשל ב-R ,

דגימה מהתפלגות אחידה סטנדרטית (n=1 runif(n, min = 0, max = 1) n=1
דגימה מהתפלגות נורמלית סטנדרטית (1 ,1) rnorm(n, mean = 0, sd = 1)
דגימה מהתפלגות מעריכית עם 1 (1 ,1) rexp(n, rate = 1)
דגימה מהתפלגות גיאומטרית (1 ,1) rgeom(n, prob
וכדומה
במקרים בהם התוכננה בה אנו משתמשים אינה כוללת פונקציה לדגימת ההתפלגות בה אנו מעוניינים אנו יכולים להשתמש בהתפלגות האחידה הסטנדרטית על מנת לדגום מההתפלגות הרצויה.

למה 1.8

יהי (1)F + U(0,1) פונקציית התפלגות מצטברת בדידה לגמרי, עם האטומים {... x} אזי משתנה המקרי

x_{i} = X + y_{i} I[F(x_{j} -) < V

הוא בעל ההתפלגות המצטברת הרצויה F.

# Page 5

הוכחה:

נשים לב כי בכל מימוש של המשתנה המקרי V יש בדיוק אינדיקטור יחיד המקבל את הערך 1 (השאר מתאפסים)

$$
\mathbb{P}\left(X=x_{j}\right)=\mathbb{P}\left(F\left(x_{j}-\right)<V \leq F\left(x_{j}\right)\right)=F\left(x_{j}\right)-F\left(x_{j}-\right)
$$

ולכן X מתפלג לפי F, כנדרש.

למה 1.9

יהי (0,1)V+ U פונקציית התפלגות מצטברת רציפה לגמרי עולה ממש.

1. אזי המשתנה המקרי

$$
X=F^{-1}(V)
$$

מתפלג לפי ההתפלגות המצטברת F.
2. יהיה X משתנה מקרי רציף לגמרי עם פונקציית התפלגות מצטברת עולה ממש G. אזי המשתנה המקרי

$$
V=G(X)
$$

מתפלג (0,1).

הוכחה:

1. לכל ,, פונקציית ההתפלגות המצטברת של X היא

$$
\mathbb{P}_{X}(X \leq x)=\mathbb{P}_{V}\left(F^{-1}(V) \leq x\right)=\mathbb{P}_{V}\left(F\left(F^{-1}(V)\right) \leq F(x)\right)=\mathbb{P}_{V}(V \leq F(x))=F(x)
$$

2. לכל ,,v, 0,1

$$
\mathbb{P}_{V}(V \leq v)=\mathbb{P}_{X}\left(G(X) \leq v\right)=\mathbb{P}_{X}\left(X \leq G^{-1}(v)=G\left(G^{-1}(v)\right)=v\right.
$$

קיבלנו איפה את ההפלגות האחידה.

הערה – הלמה בנוסח קצת אחר ניתנה בתרגיל 1 שאלה 3.

דוגמא 1.20:

ניצור דגימה מהתפלגות מעריכית עם פרמטר 0 < 3. בעזרת התפלגות אחידה. פונקציית ההתפלגות המצטברת של ההתפלגות המעריכית היא רציפה ומונוטונית עולה ממש. . לכן אם נדגום (0,1) אזי . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

