\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage[pdftex, colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref} % Added pdftex option for compatibility
\usepackage[utf8]{inputenc} % Good practice for input encoding with pdflatex
\usepackage[T1]{fontenc}    % Good practice for font encoding with pdflatex
\usepackage{lmodern}        % Using Latin Modern fonts - a good alternative to Computer Modern

\geometry{a4paper, margin=1in}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

\title{Lecture Notes: The Moment Generating Function}
\author{Probability for Statisticians - Lecture 2 Supplement}
\date{\today}

\begin{document}

\maketitle

\section{Random Variables: Unveiling the Moment Generating Function}

Within the fascinating world of probability, we often seek tools to concisely describe the behavior of random variables. One such powerful instrument is the Moment Generating Function (MGF). As its name intriguingly suggests, it provides a systematic way to 'generate' the moments (like the mean, variance, etc.) of a random variable.

\subsection{Definition: Capturing the Essence}

Let's formally define this function.

\begin{definition}[Moment Generating Function (MGF)]
Let $X$ be a random variable. Suppose there exists some positive value $\delta > 0$ such that the expected value $\mathbb{E}[e^{\delta|X|}]$ is finite ($\mathbb{E}[e^{\delta|X|}] < \infty$). Then, the \textbf{Moment Generating Function (MGF)} of $X$, denoted by $M_X(t)$, is defined as:
\[
M_X(t) = \mathbb{E}[e^{tX}]
\]
for all real numbers $t$ such that $|t| < \delta$.
\end{definition}

\begin{remark}[The Existence Condition]
The condition $\mathbb{E}[e^{\delta|X|}] < \infty$ for some $\delta > 0$ is crucial[cite: 3]. It ensures that the expectation defining the MGF, $\mathbb{E}[e^{tX}]$, converges and yields a finite value within a certain interval $(-\delta, \delta)$ around $t=0$[cite: 3]. This guarantees that the MGF is well-defined in a neighborhood of the origin, which is essential for the properties we'll explore next.
\end{remark}

\subsection{Lemma 1.5: Generating Moments Through Differentiation}

Now, let's uncover why the MGF is aptly named[cite: 2]. The following lemma reveals the connection between the MGF and the moments of the random variable $X$[cite: 4].

\begin{lemma}[Moments from the MGF] \label{lemma:mgf_moments}
Suppose the Moment Generating Function $M_X(t)$ of a random variable $X$ exists (i.e., is finite) in an open interval containing $t=0$[cite: 4]. Then, all moments of $X$, denoted by $\mathbb{E}[X^p]$ for $p = 1, 2, 3, \dots$, exist and are finite[cite: 4, 8]. Furthermore, these moments can be obtained by differentiating the MGF $M_X(t)$ with respect to $t$ and evaluating the result at $t=0$[cite: 4]:
\[
\mathbb{E}[X^p] = \left. \frac{d^p}{dt^p} M_X(t) \right|_{t=0}
\]
for all positive integers $p \in \mathbb{N}$[cite: 4].
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:mgf_moments}]
We are given that $M_X(t) = \mathbb{E}[e^{tX}]$ is defined and finite on an open interval $(-\delta, \delta)$ for some $\delta > 0$[cite: 5]. Let $p$ be any positive integer[cite: 5].

\textbf{Step 1: Show all moments $\mathbb{E}[|X|^p]$ are finite.}

Consider the Taylor series expansion of the exponential function $e^u = \sum_{j=0}^{\infty} \frac{u^j}{j!}$[cite: 5]. For any $t$ such that $0 < t < \delta$, we can write:
\[
e^{t|X|} = \sum_{j=0}^{\infty} \frac{(t|X|)^j}{j!} = 1 + t|X| + \frac{(t|X|)^2}{2!} + \dots + \frac{(t|X|)^p}{p!} + \dots
\]
Since all terms in the series are non-negative, the sum is greater than or equal to any single term. Let's pick the term where $j=p$[cite: 5]:
\[
e^{t|X|} \ge \frac{(t|X|)^p}{p!} = \frac{t^p |X|^p}{p!}
\]
Rearranging this inequality gives us[cite: 6]:
\[
|X|^p \le \frac{p!}{t^p} e^{t|X|}
\]
Now, let's take the expectation of both sides. Since $p!$ and $t^p$ are constants for a fixed $t$ and $p$, we can pull them out of the expectation[cite: 7]:
\[
\mathbb{E}[|X|^p] \le \mathbb{E}\left[ \frac{p!}{t^p} e^{t|X|} \right] = \frac{p!}{t^p} \mathbb{E}[e^{t|X|}]
\]
Since we know $M_X(t)$ exists for $t \in (-\delta, \delta)$, this implies $\mathbb{E}[e^{tX}]$ is finite[cite: 7]. A related result ensures that $\mathbb{E}[e^{t|X|}]$ is also finite for $t \in (-\delta, \delta)$[cite: 7].

Therefore, for our chosen $t$ (where $0 < t < \delta$), the term $\mathbb{E}[e^{t|X|}]$ on the right-hand side is finite[cite: 7]. Since $p!/t^p$ is also a finite constant, the entire right-hand side is finite. This forces the left-hand side, $\mathbb{E}[|X|^p]$, to be finite as well[cite: 7].
Since this holds for any positive integer $p$, we conclude that all moments of $X$ exist and are finite[cite: 8].

\textbf{Step 2: Relate derivatives of $M_X(t)$ to moments.}

Now, let's differentiate $M_X(t) = \mathbb{E}[e^{tX}]$ with respect to $t$. A key step here involves interchanging the order of differentiation and expectation[cite: 9, 10]. This is permissible precisely because we've established that the moments exist and are finite, which ensures the necessary convergence conditions are met[cite: 10].
\[
\frac{d}{dt} M_X(t) = \frac{d}{dt} \mathbb{E}[e^{tX}] = \mathbb{E}\left[ \frac{d}{dt} e^{tX} \right] = \mathbb{E}[X e^{tX}]
\]
Differentiating again:
\[
\frac{d^2}{dt^2} M_X(t) = \frac{d}{dt} \mathbb{E}[X e^{tX}] = \mathbb{E}\left[ \frac{d}{dt} (X e^{tX}) \right] = \mathbb{E}[X^2 e^{tX}]
\]
Continuing this process $p$ times, we arrive at[cite: 10]:
\[
\frac{d^p}{dt^p} M_X(t) = \mathbb{E}[X^p e^{tX}]
\]
Finally, we evaluate this $p$-th derivative at $t=0$[cite: 10]:
\[
\left. \frac{d^p}{dt^p} M_X(t) \right|_{t=0} = \left. \mathbb{E}[X^p e^{tX}] \right|_{t=0} = \mathbb{E}[X^p e^{0 \cdot X}] = \mathbb{E}[X^p \cdot 1] = \mathbb{E}[X^p]
\]
This completes the proof[cite: 10].
\end{proof}

\begin{remark}[Why is this useful?]
Lemma 1.5 provides a powerful analytical technique. If we can find the MGF of a random variable (perhaps by evaluating the expectation $\mathbb{E}[e^{tX}]$ directly from its probability density/mass function), we can then find *any* moment $\mathbb{E}[X^p]$ simply by repeated differentiation and evaluation at $t=0$[cite: 2, 4]. This is often much easier than calculating $\mathbb{E}[X^p] = \int_{-\infty}^{\infty} x^p f(x) dx$ or $\mathbb{E}[X^p] = \sum_x x^p P(X=x)$ directly, especially for higher moments (large $p$).
\end{remark}

\end{document}